{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152648ed",
   "metadata": {},
   "source": [
    "这是一个用于临时测试代码的ipynb文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90911ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "if not \"CHDIR_FLAG\" in dir():\n",
    "    os.chdir(\"../\")\n",
    "    CHDIR_FLAG = True\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2Model, Qwen2ForCausalLM\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from src.module import (\n",
    "    SkipLayerQwen2Model, \n",
    "    SkipLayerQwen2ForCausalLM,\n",
    "    ParallelQwen2Model,\n",
    "    ParallelQwen2ForCausalLM,\n",
    ")\n",
    "\n",
    "from src.pipelines.analysis import horizontal_comparison_of_forward_hook, vertical_comparison_of_forward_hook, skip_layer_generation, easy_skip_layer_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DeepseekForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c050a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_home = r\"D:\\resource\\model\\huggingface\"\n",
    "model_names = [\n",
    "    r\"Qwen\\Qwen2.5-0.5B-Instruct\",\n",
    "    r\"deepseek-ai\\DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f16d5",
   "metadata": {},
   "source": [
    "# 测试 horizontal_comparison_of_forward_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_hook_module_names = \\\n",
    "    [f\"model.layers[{i}].self_attn.q_proj\" for i in range(24)] + \\\n",
    "    [f\"model.layers[{i}].self_attn.k_proj\" for i in range(24)] + \\\n",
    "    [f\"model.layers[{i}].self_attn.v_proj\" for i in range(24)]\n",
    "\n",
    "hook_data_path_1_1 = r\"./results/strawberry-1/fhook+Qwen2.5-0.5B-Instruct+False.pt\"\n",
    "hook_data_path_1_2 = r\"./results/strawberry-1/fhook+Qwen2.5-0.5B-Instruct+True.pt\"\n",
    "hook_data_path_2_1 = r\"./results/strawberry-2/fhook+Qwen2.5-0.5B-Instruct+False.pt\"\n",
    "hook_data_path_2_2 = r\"./results/strawberry-2/fhook+Qwen2.5-0.5B-Instruct+True.pt\"\n",
    "\n",
    "table_path_1_1 = r\"./results/strawberry-1/decode+Qwen2.5-0.5B-Instruct+False.csv\"\n",
    "table_path_1_2 = r\"./results/strawberry-1/decode+Qwen2.5-0.5B-Instruct+True.csv\"\n",
    "table_path_2_1 = r\"./results/strawberry-2/decode+Qwen2.5-0.5B-Instruct+False.csv\"\n",
    "table_path_2_2 = r\"./results/strawberry-2/decode+Qwen2.5-0.5B-Instruct+True.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_comparison_of_forward_hook(\n",
    "    hook_datas = None,\n",
    "    hook_data_paths = [hook_data_path_1_2, hook_data_path_2_2],\n",
    "    hook_module_names = forward_hook_module_names[:],\n",
    "    hook_module_name_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"corr\", \"sim\", \"robust_corr\", \"robust_sim\"],\n",
    "    max_length = 4,\n",
    "    outlier_ratio = .1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4769b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_comparison_of_forward_hook(\n",
    "    hook_datas = None,\n",
    "    hook_data_paths = [hook_data_path_1_2, hook_data_path_2_2],\n",
    "    hook_module_names = forward_hook_module_names[:],\n",
    "    hook_module_name_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"corr\", \"sim\"],\n",
    "    max_length = 4,\n",
    "    outlier_ratio = .0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e64b9b",
   "metadata": {},
   "source": [
    "# 测试 vertical_comparison_of_forward_hook\n",
    "\n",
    "观察结论：\n",
    "- 第0层输入输出虽然diff很小，但是相关系数非常低，说明语义在这一层做了一个很大的变换，可能是旋转之类的，所以第0层非常重要\n",
    "- 第1层基本上的相似度也是显著低于其余层的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75baa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_data_paths = [\n",
    "    r\"./results/strawberry-X-4/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/longlong-1/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/prime-1/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/table-1/fhook+Qwen2.5-0.5B-Instruct+True-1.pt\",\n",
    "    r\"./results/strawberry-X-3/fhook+DeepSeek-R1-Distill-Qwen-1.5B+True-0.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+False-0.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+False-1.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+False-2.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+False-3.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+True-1.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+True-2.pt\",\n",
    "    r\"./results/selfattn-1/fhook+Qwen2.5-0.5B-Instruct+True-3.pt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd867e4",
   "metadata": {},
   "source": [
    "## Strawberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01abdb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vertical_comparison_of_forward_hook(\n",
    "    hook_data = None,\n",
    "    hook_data_path = hook_data_paths[0], # prime-true\n",
    "    hook_module_names = [f\"model.layers[{i}]\" for i in range(24)],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"sim\", \"robust_sim\"],\n",
    "    max_length = 16,\n",
    "    figure_size = 5,\n",
    "    watched_module_names = [f\"model.layers[{i}]\" for i in [2]],\n",
    "    outlier_ratio = .1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b2a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vertical_comparison_of_forward_hook(\n",
    "    hook_data = None,\n",
    "    hook_data_path = hook_data_paths[0], # prime-true\n",
    "    hook_module_names = [f\"model.layers[{i}]\" for i in range(24)],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"sim\", \"robust_sim\"],\n",
    "    max_length = 16,\n",
    "    figure_size = 5,\n",
    "    watched_module_names = [f\"model.layers[{i}]\" for i in []],\n",
    "    outlier_ratio = .05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536cfcf",
   "metadata": {},
   "source": [
    "## longlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e644e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vertical_comparison_of_forward_hook(\n",
    "    hook_data = None,\n",
    "    hook_data_path = hook_data_paths[1], # prime-true\n",
    "    hook_module_names = [f\"model.layers[{i}]\" for i in range(24)],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"corr\", \"sim\"],\n",
    "    max_length = 4,\n",
    "    figure_size = 5,\n",
    "    watched_module_names = [f\"model.layers[{i}]\" for i in [2, 3, 5, 20, 21, 22, 23]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2613473",
   "metadata": {},
   "source": [
    "## prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e705",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vertical_comparison_of_forward_hook(\n",
    "    hook_data = None,\n",
    "    hook_data_path = hook_data_paths[2],\n",
    "    hook_module_names = [f\"model.layers[{i}]\" for i in range(24)],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"corr\", \"sim\"],\n",
    "    max_length = 16,\n",
    "    figure_size = 5,\n",
    "    watched_module_names = [f\"model.layers[{i}]\" for i in [2, 3, 5, 20, 21, 22, 23]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c62cc",
   "metadata": {},
   "source": [
    "## table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_comparison_of_forward_hook(\n",
    "    hook_data = None,\n",
    "    hook_data_path = hook_data_paths[3], # prime-true\n",
    "    hook_module_names = [f\"model.layers[{i}]\" for i in range(24)],\n",
    "    comparison_index = [\"mean_diff\", \"max_diff\", \"corr\", \"sim\"],\n",
    "    max_length = 4,\n",
    "    figure_size = 5,\n",
    "    watched_module_names = [f\"model.layers[{i}]\" for i in [2, 3, 5, 20, 21, 22, 23]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467a042",
   "metadata": {},
   "source": [
    "# 测试skip_layer封装\n",
    "\n",
    "- 目前这两种写法对past_key_values以及hook的影响：\n",
    "\n",
    "  - past_key_values永远是num_hidden_layers的长度（DynamicCache），然后被跳过的层的past_key_values是两个(None, None)，这两个都一样\n",
    "  - hook是有区别的，forward_hook_module_names的写法是要区别的：\n",
    "    - 第一种方法（每次推理需要重新加载模型），forward_hook_module_names永远是`[\"model.layers[0]\", ..., model.layers[num_hidden_layers-1]]`，但是被跳过的那几层的hook是没有数据的（即args, kwargs, outputs都是空列表）\n",
    "    - 第二种方法（每次推理无需重新加载模型），forward_hook_module_names是不包含skip_layer_ids的层\n",
    "    - 原因是第一种方法其实只是在forward函数中重新调整的`self.layers`，整体上模型还是那么多层，但是后者则是直接把整个模型的层给扔了，整个推理完才加回来的，所以forward_hook_module_names的layer_id必须是不能包含skip_layer_ids，否则hook会报错，根本找不到那些层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7a0f4",
   "metadata": {},
   "source": [
    "## 使用重写的模型类（skip不同的层推理需要重新加载模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Model, Qwen2ForCausalLM\n",
    "model_path = os.path.join(model_home, model_names[0]) # Qwen2-0.5B\n",
    "prompt = \"解方程：x^2 - 3x + 2 = 0\"\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "results = skip_layer_generation(\n",
    "    SkipLayerModelForCausalLM = SkipLayerQwen2ForCausalLM,\n",
    "    model_name_or_path = model_path,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = prompt,\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = None,\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "print(results[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45774fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Model, Qwen2ForCausalLM\n",
    "model_path = os.path.join(model_home, model_names[0]) # Qwen2-0.5B\n",
    "prompt = \"解方程：x^2 - 3x + 2 = 0\"\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "results = skip_layer_generation(\n",
    "    SkipLayerModelForCausalLM = SkipLayerQwen2ForCausalLM,\n",
    "    model_name_or_path = model_path,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = prompt,\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = [\"model.layers[1]\"],\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "print(results[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Model, Qwen2ForCausalLM\n",
    "model_path = os.path.join(model_home, model_names[0]) # Qwen2-0.5B\n",
    "prompt = \"很久很久以前\"\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "results = skip_layer_generation(\n",
    "    SkipLayerModelForCausalLM = SkipLayerQwen2ForCausalLM,\n",
    "    model_name_or_path = model_path,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = prompt,\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0, 2],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = None,\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "print(results[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566afbb",
   "metadata": {},
   "source": [
    "## 使用easy模型（skip不同的层推理不需要重新加载模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_home, model_names[0]) # Qwen2-0.5B\n",
    "prompt = \"很久很久以前\"\n",
    "max_length = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb1d114",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight cpu\n",
      "model.layers.0.self_attn.q_proj.weight cpu\n",
      "model.layers.0.self_attn.q_proj.bias cpu\n",
      "model.layers.0.self_attn.k_proj.weight cpu\n",
      "model.layers.0.self_attn.k_proj.bias cpu\n",
      "model.layers.0.self_attn.v_proj.weight cpu\n",
      "model.layers.0.self_attn.v_proj.bias cpu\n",
      "model.layers.0.self_attn.o_proj.weight cpu\n",
      "model.layers.0.mlp.gate_proj.weight cpu\n",
      "model.layers.0.mlp.up_proj.weight cpu\n",
      "model.layers.0.mlp.down_proj.weight cpu\n",
      "model.layers.0.input_layernorm.weight cpu\n",
      "model.layers.0.post_attention_layernorm.weight cpu\n",
      "model.layers.1.self_attn.q_proj.weight cpu\n",
      "model.layers.1.self_attn.q_proj.bias cpu\n",
      "model.layers.1.self_attn.k_proj.weight cpu\n",
      "model.layers.1.self_attn.k_proj.bias cpu\n",
      "model.layers.1.self_attn.v_proj.weight cpu\n",
      "model.layers.1.self_attn.v_proj.bias cpu\n",
      "model.layers.1.self_attn.o_proj.weight cpu\n",
      "model.layers.1.mlp.gate_proj.weight cpu\n",
      "model.layers.1.mlp.up_proj.weight cpu\n",
      "model.layers.1.mlp.down_proj.weight cpu\n",
      "model.layers.1.input_layernorm.weight cpu\n",
      "model.layers.1.post_attention_layernorm.weight cpu\n",
      "model.layers.2.self_attn.q_proj.weight cpu\n",
      "model.layers.2.self_attn.q_proj.bias cpu\n",
      "model.layers.2.self_attn.k_proj.weight cpu\n",
      "model.layers.2.self_attn.k_proj.bias cpu\n",
      "model.layers.2.self_attn.v_proj.weight cpu\n",
      "model.layers.2.self_attn.v_proj.bias cpu\n",
      "model.layers.2.self_attn.o_proj.weight cpu\n",
      "model.layers.2.mlp.gate_proj.weight cpu\n",
      "model.layers.2.mlp.up_proj.weight cpu\n",
      "model.layers.2.mlp.down_proj.weight cpu\n",
      "model.layers.2.input_layernorm.weight cpu\n",
      "model.layers.2.post_attention_layernorm.weight cpu\n",
      "model.layers.3.self_attn.q_proj.weight cpu\n",
      "model.layers.3.self_attn.q_proj.bias cpu\n",
      "model.layers.3.self_attn.k_proj.weight cpu\n",
      "model.layers.3.self_attn.k_proj.bias cpu\n",
      "model.layers.3.self_attn.v_proj.weight cpu\n",
      "model.layers.3.self_attn.v_proj.bias cpu\n",
      "model.layers.3.self_attn.o_proj.weight cpu\n",
      "model.layers.3.mlp.gate_proj.weight cpu\n",
      "model.layers.3.mlp.up_proj.weight cpu\n",
      "model.layers.3.mlp.down_proj.weight cpu\n",
      "model.layers.3.input_layernorm.weight cpu\n",
      "model.layers.3.post_attention_layernorm.weight cpu\n",
      "model.layers.4.self_attn.q_proj.weight cpu\n",
      "model.layers.4.self_attn.q_proj.bias cpu\n",
      "model.layers.4.self_attn.k_proj.weight cpu\n",
      "model.layers.4.self_attn.k_proj.bias cpu\n",
      "model.layers.4.self_attn.v_proj.weight cpu\n",
      "model.layers.4.self_attn.v_proj.bias cpu\n",
      "model.layers.4.self_attn.o_proj.weight cpu\n",
      "model.layers.4.mlp.gate_proj.weight cpu\n",
      "model.layers.4.mlp.up_proj.weight cpu\n",
      "model.layers.4.mlp.down_proj.weight cpu\n",
      "model.layers.4.input_layernorm.weight cpu\n",
      "model.layers.4.post_attention_layernorm.weight cpu\n",
      "model.layers.5.self_attn.q_proj.weight cpu\n",
      "model.layers.5.self_attn.q_proj.bias cpu\n",
      "model.layers.5.self_attn.k_proj.weight cpu\n",
      "model.layers.5.self_attn.k_proj.bias cpu\n",
      "model.layers.5.self_attn.v_proj.weight cpu\n",
      "model.layers.5.self_attn.v_proj.bias cpu\n",
      "model.layers.5.self_attn.o_proj.weight cpu\n",
      "model.layers.5.mlp.gate_proj.weight cpu\n",
      "model.layers.5.mlp.up_proj.weight cpu\n",
      "model.layers.5.mlp.down_proj.weight cpu\n",
      "model.layers.5.input_layernorm.weight cpu\n",
      "model.layers.5.post_attention_layernorm.weight cpu\n",
      "model.layers.6.self_attn.q_proj.weight cpu\n",
      "model.layers.6.self_attn.q_proj.bias cpu\n",
      "model.layers.6.self_attn.k_proj.weight cpu\n",
      "model.layers.6.self_attn.k_proj.bias cpu\n",
      "model.layers.6.self_attn.v_proj.weight cpu\n",
      "model.layers.6.self_attn.v_proj.bias cpu\n",
      "model.layers.6.self_attn.o_proj.weight cpu\n",
      "model.layers.6.mlp.gate_proj.weight cpu\n",
      "model.layers.6.mlp.up_proj.weight cpu\n",
      "model.layers.6.mlp.down_proj.weight cpu\n",
      "model.layers.6.input_layernorm.weight cpu\n",
      "model.layers.6.post_attention_layernorm.weight cpu\n",
      "model.layers.7.self_attn.q_proj.weight cpu\n",
      "model.layers.7.self_attn.q_proj.bias cpu\n",
      "model.layers.7.self_attn.k_proj.weight cpu\n",
      "model.layers.7.self_attn.k_proj.bias cpu\n",
      "model.layers.7.self_attn.v_proj.weight cpu\n",
      "model.layers.7.self_attn.v_proj.bias cpu\n",
      "model.layers.7.self_attn.o_proj.weight cpu\n",
      "model.layers.7.mlp.gate_proj.weight cpu\n",
      "model.layers.7.mlp.up_proj.weight cpu\n",
      "model.layers.7.mlp.down_proj.weight cpu\n",
      "model.layers.7.input_layernorm.weight cpu\n",
      "model.layers.7.post_attention_layernorm.weight cpu\n",
      "model.layers.8.self_attn.q_proj.weight cpu\n",
      "model.layers.8.self_attn.q_proj.bias cpu\n",
      "model.layers.8.self_attn.k_proj.weight cpu\n",
      "model.layers.8.self_attn.k_proj.bias cpu\n",
      "model.layers.8.self_attn.v_proj.weight cpu\n",
      "model.layers.8.self_attn.v_proj.bias cpu\n",
      "model.layers.8.self_attn.o_proj.weight cpu\n",
      "model.layers.8.mlp.gate_proj.weight cpu\n",
      "model.layers.8.mlp.up_proj.weight cpu\n",
      "model.layers.8.mlp.down_proj.weight cpu\n",
      "model.layers.8.input_layernorm.weight cpu\n",
      "model.layers.8.post_attention_layernorm.weight cpu\n",
      "model.layers.9.self_attn.q_proj.weight cpu\n",
      "model.layers.9.self_attn.q_proj.bias cpu\n",
      "model.layers.9.self_attn.k_proj.weight cpu\n",
      "model.layers.9.self_attn.k_proj.bias cpu\n",
      "model.layers.9.self_attn.v_proj.weight cpu\n",
      "model.layers.9.self_attn.v_proj.bias cpu\n",
      "model.layers.9.self_attn.o_proj.weight cpu\n",
      "model.layers.9.mlp.gate_proj.weight cpu\n",
      "model.layers.9.mlp.up_proj.weight cpu\n",
      "model.layers.9.mlp.down_proj.weight cpu\n",
      "model.layers.9.input_layernorm.weight cpu\n",
      "model.layers.9.post_attention_layernorm.weight cpu\n",
      "model.layers.10.self_attn.q_proj.weight cpu\n",
      "model.layers.10.self_attn.q_proj.bias cpu\n",
      "model.layers.10.self_attn.k_proj.weight cpu\n",
      "model.layers.10.self_attn.k_proj.bias cpu\n",
      "model.layers.10.self_attn.v_proj.weight cpu\n",
      "model.layers.10.self_attn.v_proj.bias cpu\n",
      "model.layers.10.self_attn.o_proj.weight cpu\n",
      "model.layers.10.mlp.gate_proj.weight cpu\n",
      "model.layers.10.mlp.up_proj.weight cpu\n",
      "model.layers.10.mlp.down_proj.weight cpu\n",
      "model.layers.10.input_layernorm.weight cpu\n",
      "model.layers.10.post_attention_layernorm.weight cpu\n",
      "model.layers.11.self_attn.q_proj.weight cpu\n",
      "model.layers.11.self_attn.q_proj.bias cpu\n",
      "model.layers.11.self_attn.k_proj.weight cpu\n",
      "model.layers.11.self_attn.k_proj.bias cpu\n",
      "model.layers.11.self_attn.v_proj.weight cpu\n",
      "model.layers.11.self_attn.v_proj.bias cpu\n",
      "model.layers.11.self_attn.o_proj.weight cpu\n",
      "model.layers.11.mlp.gate_proj.weight cpu\n",
      "model.layers.11.mlp.up_proj.weight cpu\n",
      "model.layers.11.mlp.down_proj.weight cpu\n",
      "model.layers.11.input_layernorm.weight cpu\n",
      "model.layers.11.post_attention_layernorm.weight cpu\n",
      "model.layers.12.self_attn.q_proj.weight cpu\n",
      "model.layers.12.self_attn.q_proj.bias cpu\n",
      "model.layers.12.self_attn.k_proj.weight cpu\n",
      "model.layers.12.self_attn.k_proj.bias cpu\n",
      "model.layers.12.self_attn.v_proj.weight cpu\n",
      "model.layers.12.self_attn.v_proj.bias cpu\n",
      "model.layers.12.self_attn.o_proj.weight cpu\n",
      "model.layers.12.mlp.gate_proj.weight cpu\n",
      "model.layers.12.mlp.up_proj.weight cpu\n",
      "model.layers.12.mlp.down_proj.weight cpu\n",
      "model.layers.12.input_layernorm.weight cpu\n",
      "model.layers.12.post_attention_layernorm.weight cpu\n",
      "model.layers.13.self_attn.q_proj.weight cpu\n",
      "model.layers.13.self_attn.q_proj.bias cpu\n",
      "model.layers.13.self_attn.k_proj.weight cpu\n",
      "model.layers.13.self_attn.k_proj.bias cpu\n",
      "model.layers.13.self_attn.v_proj.weight cpu\n",
      "model.layers.13.self_attn.v_proj.bias cpu\n",
      "model.layers.13.self_attn.o_proj.weight cpu\n",
      "model.layers.13.mlp.gate_proj.weight cpu\n",
      "model.layers.13.mlp.up_proj.weight cpu\n",
      "model.layers.13.mlp.down_proj.weight cpu\n",
      "model.layers.13.input_layernorm.weight cpu\n",
      "model.layers.13.post_attention_layernorm.weight cpu\n",
      "model.layers.14.self_attn.q_proj.weight cpu\n",
      "model.layers.14.self_attn.q_proj.bias cpu\n",
      "model.layers.14.self_attn.k_proj.weight cpu\n",
      "model.layers.14.self_attn.k_proj.bias cpu\n",
      "model.layers.14.self_attn.v_proj.weight cpu\n",
      "model.layers.14.self_attn.v_proj.bias cpu\n",
      "model.layers.14.self_attn.o_proj.weight cpu\n",
      "model.layers.14.mlp.gate_proj.weight cpu\n",
      "model.layers.14.mlp.up_proj.weight cpu\n",
      "model.layers.14.mlp.down_proj.weight cpu\n",
      "model.layers.14.input_layernorm.weight cpu\n",
      "model.layers.14.post_attention_layernorm.weight cpu\n",
      "model.layers.15.self_attn.q_proj.weight cpu\n",
      "model.layers.15.self_attn.q_proj.bias cpu\n",
      "model.layers.15.self_attn.k_proj.weight cpu\n",
      "model.layers.15.self_attn.k_proj.bias cpu\n",
      "model.layers.15.self_attn.v_proj.weight cpu\n",
      "model.layers.15.self_attn.v_proj.bias cpu\n",
      "model.layers.15.self_attn.o_proj.weight cpu\n",
      "model.layers.15.mlp.gate_proj.weight cpu\n",
      "model.layers.15.mlp.up_proj.weight cpu\n",
      "model.layers.15.mlp.down_proj.weight cpu\n",
      "model.layers.15.input_layernorm.weight cpu\n",
      "model.layers.15.post_attention_layernorm.weight cpu\n",
      "model.layers.16.self_attn.q_proj.weight cpu\n",
      "model.layers.16.self_attn.q_proj.bias cpu\n",
      "model.layers.16.self_attn.k_proj.weight cpu\n",
      "model.layers.16.self_attn.k_proj.bias cpu\n",
      "model.layers.16.self_attn.v_proj.weight cpu\n",
      "model.layers.16.self_attn.v_proj.bias cpu\n",
      "model.layers.16.self_attn.o_proj.weight cpu\n",
      "model.layers.16.mlp.gate_proj.weight cpu\n",
      "model.layers.16.mlp.up_proj.weight cpu\n",
      "model.layers.16.mlp.down_proj.weight cpu\n",
      "model.layers.16.input_layernorm.weight cpu\n",
      "model.layers.16.post_attention_layernorm.weight cpu\n",
      "model.layers.17.self_attn.q_proj.weight cpu\n",
      "model.layers.17.self_attn.q_proj.bias cpu\n",
      "model.layers.17.self_attn.k_proj.weight cpu\n",
      "model.layers.17.self_attn.k_proj.bias cpu\n",
      "model.layers.17.self_attn.v_proj.weight cpu\n",
      "model.layers.17.self_attn.v_proj.bias cpu\n",
      "model.layers.17.self_attn.o_proj.weight cpu\n",
      "model.layers.17.mlp.gate_proj.weight cpu\n",
      "model.layers.17.mlp.up_proj.weight cpu\n",
      "model.layers.17.mlp.down_proj.weight cpu\n",
      "model.layers.17.input_layernorm.weight cpu\n",
      "model.layers.17.post_attention_layernorm.weight cpu\n",
      "model.layers.18.self_attn.q_proj.weight cpu\n",
      "model.layers.18.self_attn.q_proj.bias cpu\n",
      "model.layers.18.self_attn.k_proj.weight cpu\n",
      "model.layers.18.self_attn.k_proj.bias cpu\n",
      "model.layers.18.self_attn.v_proj.weight cpu\n",
      "model.layers.18.self_attn.v_proj.bias cpu\n",
      "model.layers.18.self_attn.o_proj.weight cpu\n",
      "model.layers.18.mlp.gate_proj.weight cpu\n",
      "model.layers.18.mlp.up_proj.weight cpu\n",
      "model.layers.18.mlp.down_proj.weight cpu\n",
      "model.layers.18.input_layernorm.weight cpu\n",
      "model.layers.18.post_attention_layernorm.weight cpu\n",
      "model.layers.19.self_attn.q_proj.weight cpu\n",
      "model.layers.19.self_attn.q_proj.bias cpu\n",
      "model.layers.19.self_attn.k_proj.weight cpu\n",
      "model.layers.19.self_attn.k_proj.bias cpu\n",
      "model.layers.19.self_attn.v_proj.weight cpu\n",
      "model.layers.19.self_attn.v_proj.bias cpu\n",
      "model.layers.19.self_attn.o_proj.weight cpu\n",
      "model.layers.19.mlp.gate_proj.weight cpu\n",
      "model.layers.19.mlp.up_proj.weight cpu\n",
      "model.layers.19.mlp.down_proj.weight cpu\n",
      "model.layers.19.input_layernorm.weight cpu\n",
      "model.layers.19.post_attention_layernorm.weight cpu\n",
      "model.layers.20.self_attn.q_proj.weight cpu\n",
      "model.layers.20.self_attn.q_proj.bias cpu\n",
      "model.layers.20.self_attn.k_proj.weight cpu\n",
      "model.layers.20.self_attn.k_proj.bias cpu\n",
      "model.layers.20.self_attn.v_proj.weight cpu\n",
      "model.layers.20.self_attn.v_proj.bias cpu\n",
      "model.layers.20.self_attn.o_proj.weight cpu\n",
      "model.layers.20.mlp.gate_proj.weight cpu\n",
      "model.layers.20.mlp.up_proj.weight cpu\n",
      "model.layers.20.mlp.down_proj.weight cpu\n",
      "model.layers.20.input_layernorm.weight cpu\n",
      "model.layers.20.post_attention_layernorm.weight cpu\n",
      "model.layers.21.self_attn.q_proj.weight cpu\n",
      "model.layers.21.self_attn.q_proj.bias cpu\n",
      "model.layers.21.self_attn.k_proj.weight cpu\n",
      "model.layers.21.self_attn.k_proj.bias cpu\n",
      "model.layers.21.self_attn.v_proj.weight cpu\n",
      "model.layers.21.self_attn.v_proj.bias cpu\n",
      "model.layers.21.self_attn.o_proj.weight cpu\n",
      "model.layers.21.mlp.gate_proj.weight cpu\n",
      "model.layers.21.mlp.up_proj.weight cpu\n",
      "model.layers.21.mlp.down_proj.weight cpu\n",
      "model.layers.21.input_layernorm.weight cpu\n",
      "model.layers.21.post_attention_layernorm.weight cpu\n",
      "model.layers.22.self_attn.q_proj.weight cpu\n",
      "model.layers.22.self_attn.q_proj.bias cpu\n",
      "model.layers.22.self_attn.k_proj.weight cpu\n",
      "model.layers.22.self_attn.k_proj.bias cpu\n",
      "model.layers.22.self_attn.v_proj.weight cpu\n",
      "model.layers.22.self_attn.v_proj.bias cpu\n",
      "model.layers.22.self_attn.o_proj.weight cpu\n",
      "model.layers.22.mlp.gate_proj.weight cpu\n",
      "model.layers.22.mlp.up_proj.weight cpu\n",
      "model.layers.22.mlp.down_proj.weight cpu\n",
      "model.layers.22.input_layernorm.weight cpu\n",
      "model.layers.22.post_attention_layernorm.weight cpu\n",
      "model.layers.23.self_attn.q_proj.weight cpu\n",
      "model.layers.23.self_attn.q_proj.bias cpu\n",
      "model.layers.23.self_attn.k_proj.weight cpu\n",
      "model.layers.23.self_attn.k_proj.bias cpu\n",
      "model.layers.23.self_attn.v_proj.weight cpu\n",
      "model.layers.23.self_attn.v_proj.bias cpu\n",
      "model.layers.23.self_attn.o_proj.weight cpu\n",
      "model.layers.23.mlp.gate_proj.weight cpu\n",
      "model.layers.23.mlp.up_proj.weight cpu\n",
      "model.layers.23.mlp.down_proj.weight cpu\n",
      "model.layers.23.input_layernorm.weight cpu\n",
      "model.layers.23.post_attention_layernorm.weight cpu\n",
      "model.norm.weight cpu\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = easy_skip_layer_generation(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = prompt,\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0, 2],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = None,\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "results[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = easy_skip_layer_generation(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = \"解方程：x^2 - 3x + 2 = 0\",\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0],\n",
    "    use_kv_cache = False,\n",
    "    forward_hook_module_names = None,\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "results[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = easy_skip_layer_generation(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = \"解方程：x^2 - 3x + 2 = 0\",\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = None,\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "results[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = easy_skip_layer_generation(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    prompt = \"解方程：x^2 - 3x + 2 = 0\",\n",
    "    max_length = max_length,\n",
    "    skip_layer_ids = [0],\n",
    "    use_kv_cache = True,\n",
    "    forward_hook_module_names = [\"model.layers[0]\"],\n",
    "    backward_hook_module_names = None,\n",
    ")\n",
    "results[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"forward_hook_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72e38e",
   "metadata": {},
   "source": [
    "# 测试Qwen模型并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98305f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "# @author: caoyang\n",
    "# @email: caoyang@stu.sufe.edu.cn\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2Model, Qwen2ForCausalLM\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "\n",
    "# Overwrite according to /transformers/models/qwen2/modeling_qwen2.py\n",
    "# Version transformers 4.56.3\n",
    "class ParallelQwen2Model(Qwen2Model):\n",
    "    def __init__(self, config, n_cuda = 2, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self._module_to_device()\n",
    "\n",
    "    def _module_to_device(self):\n",
    "        self.embed_tokens = self.embed_tokens.to(\"cuda:0\")\n",
    "        self.norm = self.norm.to(f\"cuda:{self.n_cuda - 1}\")\n",
    "        n_layers = len(self.layers)\n",
    "        self.layer_to_device = dict()\n",
    "        for layer_id in range(n_layers):\n",
    "            for device_id in range(self.n_cuda):\n",
    "                if layer_id <= (device_id + 1) * n_layers // self.n_cuda:\n",
    "                    self.layers[layer_id] = self.layers[layer_id].to(f\"cuda:{device_id}\")\n",
    "                    self.layer_id_to_device[layer_id] = device_id\n",
    "                    break\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids = None,\n",
    "                attention_mask = None,\n",
    "                position_ids= None,\n",
    "                past_key_values = None,\n",
    "                inputs_embeds = None,\n",
    "                use_cache = None,\n",
    "                cache_position = None,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, \n",
    "                past_seen_tokens + inputs_embeds.shape[1], \n",
    "                device = inputs_embeds.device,\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "        # It may already have been prepared by e.g. `generate`\n",
    "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "            # Prepare mask arguments\n",
    "            mask_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"input_embeds\": inputs_embeds,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"position_ids\": position_ids,\n",
    "            }\n",
    "            # Create the masks\n",
    "            causal_mask_mapping = {\n",
    "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
    "            }\n",
    "            # The sliding window alternating layers are not always activated depending on the config\n",
    "            if self.has_sliding_layers:\n",
    "                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n",
    "        hidden_states = inputs_embeds\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        for layer_id, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):\n",
    "            current_device_id = self.layer_to_device[layer_id]\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                **kwargs,\n",
    "            )\n",
    "            if layer_id < self.config.num_hidden_layers - 1:\n",
    "                next_device_id = self.layer_to_device[layer_id]\n",
    "                if not current_device_id == next_device_id:\n",
    "                    next_device_name = f\"cuda:{next_device_id}\"\n",
    "                    hidden_states = hidden_states.to(next_device_name)\n",
    "                    attention_mask = causal_mask_mapping[decoder_layer.attention_type].to(next_device_name)\n",
    "                    position_ids = position_ids.to(next_device_name)\n",
    "                    cache_position = cache_position.to(next_device_name)\n",
    "                    # Deal with KV-Cache\n",
    "                    if past_key_values is not None:\n",
    "                        for i in range(past_key_values):\n",
    "                            if past_key_values[i][0] is not None:\n",
    "                                past_key_values[i][0] = past_key_values[i][0].to(next_device_name)\n",
    "                            if past_key_values[i][1] is not None:\n",
    "                                past_key_values[i][1] = past_key_values[i][1].to(next_device_name)\n",
    "        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state = hidden_states,\n",
    "            past_key_values = past_key_values if use_cache else None,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fe4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\resource\\model\\huggingface\\Qwen\\Qwen2.5-0.5B-Instruct\"\n",
    "model = ParallelQwen2Model.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980985f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c4429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myopenai_jupyter",
   "language": "python",
   "name": "myopenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

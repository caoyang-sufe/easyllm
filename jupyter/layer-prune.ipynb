{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f3995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "if not \"CHDIR_FLAG\" in dir():\n",
    "    os.chdir(\"../\")\n",
    "    CHDIR_FLAG = True\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from safetensors import safe_open\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from src.tools.plot import plot_tensor_mean_and_variance, plot_tensor_histogram, plot_tensor_heatmap\n",
    "from src.tools.torch import register_forward_hook_decorator, register_backward_hook_decorator\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e24b0b",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eef3e",
   "metadata": {},
   "source": [
    "1. 在之前的研究中，我们发现靠后的层具有更重要的作用？是否有可能丢弃一些不重要的层？\n",
    "2. 同理在之前针对LoRA块的奇异值分析中发现，某些情况下，v_proj 的确是低秩的，远远不如 q_proj 和 k_proj，猜想为 v_proj 可能不值得调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9dbad",
   "metadata": {},
   "source": [
    "# 测试Skip-layer猜想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff209ef",
   "metadata": {},
   "source": [
    "Skip-Layer:\n",
    "\n",
    "- 因为中间层的“Δ输入”以及“Δ输出”（两者本质上是同一个，只是相差一个相位）比较小，或许可以跳过其中的直接到最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8dd74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_home = r\"D:\\resource\\model\\huggingface\"\n",
    "model_names = [\n",
    "    r\"Qwen\\Qwen2.5-0.5B-Instruct\",\n",
    "    r\"deepseek-ai\\DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "]\n",
    "hook_data_paths = [\n",
    "    r\"./results/strawberry-X-4/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/longlong-1/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/prime-1/fhook+Qwen2.5-0.5B-Instruct+True-0.pt\",\n",
    "    r\"./results/table-1/fhook+Qwen2.5-0.5B-Instruct+True-1.pt\",\n",
    "    r\"./results/strawberry-X-3/fhook+DeepSeek-R1-Distill-Qwen-1.5B+True-0.pt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18df47a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(os.path.join(model_home, model_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0919ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_hook_module_names = [f\"model.layers[{i}]\" for i in range(24)]\n",
    "hook_data = torch.load(hook_data_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3632ec72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-6.1340e-03,  1.1230e-02,  2.1210e-03, -4.9744e-03, -3.6926e-03,\n",
       "            8.0566e-03,  3.4943e-03, -8.0566e-03, -3.2349e-03,  2.0981e-04,\n",
       "           -9.8877e-03, -1.4343e-03, -2.9144e-03, -1.3855e-02,  1.6968e-02,\n",
       "            6.0120e-03,  1.5259e-02,  9.8877e-03,  9.7656e-03, -7.4158e-03,\n",
       "            2.7832e-02,  1.3245e-02,  7.9956e-03,  3.0670e-03, -9.1553e-04,\n",
       "            6.9275e-03,  3.5095e-03, -4.7607e-03, -3.7384e-03,  4.9744e-03,\n",
       "            9.6436e-03, -1.0559e-02, -7.8201e-05,  1.6861e-03,  1.0620e-02,\n",
       "            2.8534e-03, -2.1935e-04, -5.7068e-03, -1.2756e-02,  5.2795e-03,\n",
       "            1.1536e-02,  1.7071e-04, -1.4038e-03,  6.8970e-03, -2.3270e-04,\n",
       "           -2.2736e-03, -7.2632e-03, -7.6904e-03,  2.3560e-02, -1.3306e-02,\n",
       "            8.0566e-03,  7.7820e-03,  1.0681e-02, -1.1902e-02,  7.2327e-03,\n",
       "            8.7891e-03, -2.2217e-02,  2.0020e-02, -4.3030e-03,  2.9175e-02,\n",
       "            2.2888e-03,  8.4686e-04, -7.3242e-03, -6.9275e-03, -7.0190e-03,\n",
       "            1.5015e-02, -1.5076e-02,  6.9275e-03, -2.0874e-02,  2.4414e-02,\n",
       "            2.5146e-02,  3.9978e-03, -1.0925e-02,  1.4893e-02, -1.7624e-03,\n",
       "            2.5787e-03, -1.2573e-02,  2.2949e-02, -7.8125e-03,  1.8539e-03,\n",
       "            8.1787e-03, -2.6093e-03, -2.2736e-03,  1.5717e-03,  7.8735e-03,\n",
       "            2.6398e-03,  5.4016e-03,  9.7656e-04,  1.0986e-02,  3.7689e-03,\n",
       "            7.8735e-03, -8.7280e-03,  8.3542e-04,  1.2894e-03,  5.8746e-04,\n",
       "            2.5940e-04,  7.7820e-03,  1.0559e-02, -9.2163e-03, -6.2561e-03,\n",
       "           -1.4893e-02, -1.1353e-02,  1.0315e-02, -8.6060e-03,  1.1475e-02,\n",
       "            1.5381e-02, -3.2471e-02, -9.2163e-03,  9.3384e-03, -4.5166e-03,\n",
       "           -1.5259e-02, -1.2512e-02, -1.2207e-02,  1.2146e-02, -7.6294e-03,\n",
       "           -1.2024e-02, -1.8082e-03,  1.2970e-04, -3.7384e-03,  1.1108e-02,\n",
       "           -2.2430e-03,  2.1118e-02, -9.7046e-03,  5.6763e-03, -2.2583e-03,\n",
       "           -2.5482e-03, -9.3384e-03, -7.9346e-03, -9.8267e-03,  7.7820e-03,\n",
       "           -1.5259e-03, -6.2256e-03,  8.4229e-03, -3.0365e-03,  1.6113e-02,\n",
       "           -1.4526e-02,  1.2451e-02, -1.0498e-02, -3.1494e-02,  1.8463e-03,\n",
       "           -6.4392e-03,  1.5076e-02, -4.7302e-03,  1.2634e-02, -3.4637e-03,\n",
       "           -1.9043e-02, -8.7891e-03,  9.3842e-04,  1.0376e-02, -2.1240e-02,\n",
       "           -3.4790e-03, -7.9346e-03,  4.6692e-03, -8.2397e-03, -1.4709e-02,\n",
       "            4.9438e-03, -3.6926e-03,  3.5286e-04,  5.4626e-03,  1.6846e-02,\n",
       "           -2.4567e-03, -2.0752e-02,  1.1353e-02,  3.7842e-03, -1.3855e-02,\n",
       "           -1.4771e-02, -2.9602e-03,  2.2278e-03,  1.8158e-03, -5.7617e-02,\n",
       "           -5.7983e-04,  1.7212e-02,  1.2436e-03,  1.8921e-02,  5.0659e-03,\n",
       "           -4.5471e-03, -1.0193e-02, -2.1362e-03,  2.7313e-03, -8.3618e-03,\n",
       "            5.3101e-03,  5.0354e-03,  4.5471e-03,  6.2866e-03,  1.9989e-03,\n",
       "            1.9287e-02, -8.7738e-04,  1.1353e-02, -1.7944e-02,  1.3306e-02,\n",
       "           -1.6113e-02,  5.1880e-03,  5.5542e-03, -5.7068e-03,  7.2937e-03,\n",
       "           -2.2125e-04,  1.9287e-02, -1.2451e-02,  4.8828e-03,  9.5215e-03,\n",
       "            7.5150e-04, -6.7520e-04, -4.6387e-03, -1.0986e-02, -1.9897e-02,\n",
       "           -3.1281e-03,  3.7079e-03, -2.0142e-03,  3.7537e-03, -2.6855e-03,\n",
       "            7.5684e-03,  5.1575e-03,  9.7656e-03,  1.0864e-02,  2.1851e-02,\n",
       "            1.8616e-03, -1.1169e-02, -5.3024e-04,  2.2888e-03, -2.2217e-02,\n",
       "            1.2970e-03,  1.5503e-02, -8.7891e-03, -1.0193e-02,  9.7656e-03,\n",
       "           -1.0864e-02,  1.2451e-02,  7.1106e-03,  9.1553e-03, -3.0823e-03,\n",
       "           -1.9409e-02,  6.5002e-03, -1.9653e-02,  8.0872e-04,  1.6556e-03,\n",
       "           -5.9814e-03,  3.3875e-03,  2.7161e-03,  1.8845e-03,  1.0925e-02,\n",
       "           -4.7913e-03,  9.5825e-03,  6.5613e-03,  1.5503e-02, -2.2736e-03,\n",
       "           -1.4893e-02,  2.3346e-03,  8.9722e-03, -1.4526e-02,  5.0354e-03,\n",
       "            1.4954e-02, -3.7231e-03, -4.3640e-03, -1.4648e-02,  7.2327e-03,\n",
       "            7.5684e-03,  1.8921e-02,  1.0925e-02, -1.6602e-02, -5.0735e-04,\n",
       "            1.3367e-02,  5.5542e-03, -1.0986e-02,  6.1646e-03, -1.5747e-02,\n",
       "            4.2725e-03,  5.0354e-03, -1.4954e-02,  3.0640e-02, -2.5391e-02,\n",
       "           -6.4087e-03, -1.6479e-02,  2.1606e-02, -6.8054e-03,  7.5378e-03,\n",
       "            3.2806e-03, -4.6082e-03,  1.3123e-02,  1.1475e-02,  8.6212e-04,\n",
       "            1.8463e-03, -7.8125e-03, -1.7578e-02,  2.8809e-02,  3.0365e-03,\n",
       "            3.9978e-03, -1.4038e-02,  3.1891e-03, -1.3550e-02,  9.6436e-03,\n",
       "           -8.9722e-03,  7.2632e-03,  6.5918e-03,  7.7057e-04,  5.7373e-03,\n",
       "           -5.4169e-04,  5.3101e-03, -1.7700e-03,  1.6479e-02, -6.0120e-03,\n",
       "            1.3306e-02,  6.9427e-04,  3.0273e-02,  1.5564e-02, -3.8086e-02,\n",
       "           -1.1108e-02,  2.4780e-02, -2.0264e-02, -1.3367e-02,  8.2397e-03,\n",
       "            4.1199e-03,  9.8267e-03, -5.1575e-03, -1.3550e-02,  3.1891e-03,\n",
       "            5.2490e-03, -2.4567e-03,  1.4404e-02,  5.8899e-03,  7.1716e-03,\n",
       "            7.1335e-04, -3.6011e-03,  1.5076e-02,  1.4099e-02, -6.7139e-03,\n",
       "            1.5381e-02, -8.6670e-03, -3.1433e-03,  1.7578e-02,  1.1780e-02,\n",
       "            5.7678e-03,  8.1177e-03,  2.8687e-02, -2.7466e-03,  2.1240e-02,\n",
       "            2.7771e-03,  1.2634e-02, -8.9722e-03, -3.9062e-03,  4.1504e-03,\n",
       "           -2.1484e-02, -9.1553e-03,  8.0566e-03,  1.1475e-02, -1.3733e-02,\n",
       "            1.4954e-03, -2.9297e-02,  3.9978e-03, -1.3550e-02,  1.6708e-03,\n",
       "            2.9297e-03, -7.5989e-03,  2.7466e-02,  1.7822e-02, -2.1973e-03,\n",
       "            5.2795e-03, -1.1902e-02, -1.4526e-02,  1.4725e-03,  7.1106e-03,\n",
       "            1.8677e-02, -1.1963e-02, -4.2725e-03, -2.1839e-04, -9.0027e-04,\n",
       "           -3.7384e-04, -5.4321e-03,  1.0681e-02,  2.2217e-02, -4.8828e-03,\n",
       "            1.4954e-02, -1.7090e-03, -1.3977e-02, -1.9836e-03,  4.1504e-03,\n",
       "            3.9978e-03,  3.5400e-03, -2.5177e-03,  7.3242e-04,  2.8809e-02,\n",
       "           -3.1494e-02, -8.6670e-03,  5.7678e-03, -2.8198e-02, -1.2390e-02,\n",
       "            2.4658e-02,  7.6294e-03,  9.2773e-03,  1.1780e-02,  2.1820e-03,\n",
       "           -2.3499e-03, -6.6528e-03, -1.1292e-02,  7.6904e-03, -1.8311e-02,\n",
       "            5.2795e-03,  1.1414e-02, -2.2339e-02,  1.4343e-02, -5.4932e-03,\n",
       "           -3.2715e-02, -2.3071e-02, -8.3008e-03,  1.6479e-03,  5.1880e-03,\n",
       "            1.0071e-02, -1.6327e-03,  1.1597e-03,  3.9978e-03,  9.6436e-03,\n",
       "           -5.2490e-03,  3.8147e-03,  1.4648e-02, -1.7822e-02,  9.5215e-03,\n",
       "            1.3367e-02,  4.4861e-03,  1.1826e-03, -1.7166e-03, -1.5991e-02,\n",
       "            5.1270e-03,  8.6975e-04, -1.5259e-03,  6.7444e-03, -5.2185e-03,\n",
       "           -2.0630e-02, -9.1553e-03, -1.1414e-02, -8.4305e-04, -6.1340e-03,\n",
       "            2.4658e-02,  1.4526e-02, -2.2949e-02,  1.5564e-02, -1.7944e-02,\n",
       "           -9.5367e-04, -2.2583e-02, -3.0670e-03, -3.4332e-03, -6.3171e-03,\n",
       "            5.1270e-03, -1.3855e-02,  2.0020e-02,  3.7537e-03,  1.9897e-02,\n",
       "           -5.7983e-04,  1.0010e-02,  1.0620e-02, -1.5163e-04,  2.4414e-03,\n",
       "           -4.4556e-03, -9.6436e-03, -9.9487e-03,  1.4709e-02, -1.2939e-02,\n",
       "            8.9111e-03,  2.5787e-03,  1.3977e-02,  2.0142e-02,  2.8534e-03,\n",
       "           -1.0300e-03,  3.6469e-03, -7.8125e-03, -5.8289e-03,  7.3547e-03,\n",
       "           -1.1597e-02,  1.4771e-02,  7.5378e-03, -1.4038e-02, -2.0264e-02,\n",
       "            9.2773e-03, -8.6594e-04,  5.3711e-03,  9.3384e-03,  1.3184e-02,\n",
       "            1.4099e-02, -1.1841e-02,  8.3618e-03, -3.2959e-03,  1.2817e-03,\n",
       "           -8.8120e-04,  3.0670e-03, -6.9580e-03,  8.6060e-03,  1.2146e-02,\n",
       "           -4.9744e-03,  1.2817e-03,  2.1973e-03, -1.5991e-02,  1.1414e-02,\n",
       "            5.2002e-02, -1.0803e-02,  7.1716e-03, -2.6703e-03, -1.6251e-03,\n",
       "            8.1787e-03,  7.3242e-03, -1.0803e-02, -9.3994e-03,  5.0049e-03,\n",
       "            1.9409e-02, -1.0559e-02, -1.0452e-03,  6.5308e-03, -9.3937e-05,\n",
       "            3.2501e-03,  3.6316e-03, -1.3306e-02,  1.3275e-03, -1.3489e-02,\n",
       "            4.3640e-03, -4.1504e-03, -1.5381e-02, -1.0803e-02,  6.1646e-03,\n",
       "           -5.3101e-03, -1.3184e-02, -2.5024e-02, -1.3199e-03,  1.3306e-02,\n",
       "           -8.1177e-03,  7.6904e-03, -2.7710e-02, -8.0566e-03,  1.9897e-02,\n",
       "           -1.6479e-02,  1.3550e-02,  9.0332e-03, -2.1484e-02,  1.0681e-02,\n",
       "            7.9346e-03,  8.7280e-03, -6.8970e-03,  9.6436e-03, -3.1738e-03,\n",
       "           -7.9346e-03,  6.2561e-03,  1.2085e-02,  7.7820e-03, -7.8125e-03,\n",
       "           -1.5747e-02, -9.1553e-03,  1.4709e-02,  1.5625e-02, -6.9885e-03,\n",
       "           -6.8665e-04,  1.3123e-02, -1.3916e-02,  1.3733e-03,  6.6833e-03,\n",
       "            1.4160e-02,  6.7139e-03,  1.8799e-02, -1.0254e-02,  3.1982e-02,\n",
       "            6.9275e-03, -1.5198e-02, -1.8311e-02,  1.2268e-02,  6.1340e-03,\n",
       "           -6.5613e-03,  7.5378e-03, -2.6703e-03, -3.0823e-03,  2.9602e-03,\n",
       "            1.3962e-03,  7.2021e-03, -6.7139e-03, -7.0190e-03,  1.1169e-02,\n",
       "           -2.4780e-02,  6.5308e-03, -2.9297e-02, -6.3477e-03,  1.1230e-02,\n",
       "           -1.5259e-02, -4.8523e-03,  1.0986e-02, -3.6163e-03,  7.5073e-03,\n",
       "            4.9438e-03,  1.0834e-03,  7.2327e-03, -6.2866e-03,  1.1353e-02,\n",
       "            1.2390e-02, -1.8387e-03, -6.6223e-03, -7.6294e-03, -1.5320e-02,\n",
       "           -2.3956e-03,  1.5869e-02,  1.7395e-03, -1.7548e-03, -1.0681e-03,\n",
       "           -3.0273e-02,  5.2185e-03, -9.3384e-03, -1.3855e-02,  7.2632e-03,\n",
       "           -1.2939e-02, -1.4465e-02, -2.3071e-02,  1.2939e-02,  1.6937e-03,\n",
       "            7.4768e-03, -2.0020e-02,  9.2773e-03,  2.3499e-03,  8.4229e-03,\n",
       "           -1.1414e-02,  1.8768e-03, -2.2949e-02, -5.1270e-03, -1.4343e-03,\n",
       "           -2.7954e-02,  7.8735e-03, -9.2163e-03,  5.3711e-03, -1.4160e-02,\n",
       "           -3.7079e-03, -2.5787e-03, -1.4954e-03,  5.6458e-03,  3.7537e-03,\n",
       "            5.7068e-03,  1.4221e-02,  3.1433e-03,  4.9438e-03,  6.8665e-03,\n",
       "            1.3123e-03,  2.0599e-03, -3.8338e-04,  7.3853e-03, -1.0803e-02,\n",
       "            1.5182e-03, -1.0681e-02, -2.2430e-03,  1.9409e-02, -7.3547e-03,\n",
       "           -1.2329e-02, -1.8677e-02, -5.3101e-03,  8.1253e-04,  1.4420e-03,\n",
       "            1.5869e-02, -1.9653e-02,  1.9165e-02,  8.4229e-03, -2.1851e-02,\n",
       "            1.7776e-03,  8.9722e-03,  1.4404e-02, -5.4321e-03, -2.8076e-03,\n",
       "           -4.1771e-04, -5.3406e-03, -8.4839e-03, -2.4567e-03,  2.0874e-02,\n",
       "           -3.8300e-03,  7.5989e-03,  6.8665e-04, -9.3384e-03, -1.2894e-03,\n",
       "           -4.9438e-03,  1.3000e-02,  1.1780e-02, -4.6997e-03, -1.0498e-02,\n",
       "           -9.7656e-03,  6.3171e-03, -1.8188e-02, -1.9897e-02,  4.6997e-03,\n",
       "           -4.2114e-03, -1.4832e-02, -3.8147e-04, -1.2756e-02,  1.1047e-02,\n",
       "            1.5640e-03, -1.0300e-03, -1.8311e-02, -5.4321e-03,  1.7578e-02,\n",
       "           -5.5542e-03, -6.7139e-04,  7.3547e-03, -9.4604e-03, -1.2939e-02,\n",
       "            1.4526e-02, -7.3853e-03, -1.1353e-02,  9.5825e-03, -5.7068e-03,\n",
       "            1.1292e-02,  3.0060e-03, -1.0986e-02, -6.6833e-03, -2.9755e-04,\n",
       "           -9.8877e-03,  2.3804e-03, -1.0132e-02, -1.7700e-02, -7.6599e-03,\n",
       "           -3.2654e-03, -1.3275e-03,  4.1809e-03,  2.5635e-03, -2.0996e-02,\n",
       "            1.1292e-02, -8.5449e-03,  5.0049e-03, -5.0659e-03,  2.6855e-02,\n",
       "            1.0254e-02,  3.6926e-03,  5.9509e-03,  1.7578e-02, -1.3916e-02,\n",
       "            7.5378e-03,  9.6130e-04,  8.7738e-04, -1.2146e-02, -1.0437e-02,\n",
       "           -7.7515e-03, -4.8523e-03, -1.1597e-02,  8.1787e-03, -3.6049e-04,\n",
       "            2.1515e-03,  1.2329e-02,  1.8677e-02,  7.7515e-03,  1.1902e-02,\n",
       "            1.0803e-02, -6.7444e-03, -4.3335e-03,  1.3977e-02,  1.0071e-02,\n",
       "           -8.1177e-03,  1.4282e-02,  1.5182e-03, -6.1951e-03,  2.2430e-03,\n",
       "           -1.2283e-03,  3.7384e-04, -2.0294e-03, -5.2185e-03, -2.9449e-03,\n",
       "           -1.1292e-02, -1.2756e-02, -2.3499e-03, -6.3171e-03,  3.9673e-03,\n",
       "            4.5776e-03, -9.9487e-03,  9.5215e-03, -3.5400e-03,  1.3657e-03,\n",
       "            3.4332e-04, -4.9438e-03,  1.0681e-02,  6.2561e-04, -2.2583e-03,\n",
       "            2.4658e-02,  9.4604e-03, -2.4170e-02,  4.3631e-05,  2.3499e-03,\n",
       "            1.2207e-02, -1.5015e-02, -8.6060e-03,  5.3406e-03,  1.9897e-02,\n",
       "            7.5684e-03,  8.2397e-03,  2.1362e-02, -7.3242e-03,  3.5095e-03,\n",
       "            3.3188e-04, -2.3956e-03, -1.1978e-03, -1.4404e-02, -5.5847e-03,\n",
       "           -1.0742e-02,  2.0630e-02,  1.4801e-03,  7.2937e-03, -8.9111e-03,\n",
       "            5.6763e-03, -1.5869e-02,  4.3678e-04,  9.7046e-03, -1.1902e-02,\n",
       "            3.6621e-02,  1.3184e-02,  7.0496e-03,  4.5166e-03, -6.9885e-03,\n",
       "           -9.7656e-04,  5.8365e-04, -4.7913e-03, -2.8076e-03,  4.0436e-04,\n",
       "           -1.5747e-02, -8.1177e-03, -2.3956e-03, -8.3618e-03, -7.5684e-03,\n",
       "            5.4626e-03,  4.0588e-03,  6.1951e-03, -3.6316e-03, -1.2329e-02,\n",
       "            8.6670e-03,  1.7456e-02, -7.8735e-03, -5.6763e-03, -1.4267e-03,\n",
       "           -8.7891e-03,  5.8594e-03,  2.2888e-03, -1.2146e-02, -2.1729e-02,\n",
       "           -6.6757e-04, -1.0605e-03, -1.2817e-02, -1.2756e-02, -3.4637e-03,\n",
       "           -3.4180e-03, -1.2970e-04,  9.0942e-03, -1.6602e-02, -3.3569e-03,\n",
       "           -2.2461e-02, -2.8992e-03,  5.9204e-03, -2.8442e-02, -3.6774e-03,\n",
       "            5.9509e-03, -5.5313e-04,  7.4768e-03,  1.9409e-02,  1.5015e-02,\n",
       "           -2.2888e-03,  1.6968e-02, -7.5989e-03,  7.5989e-03,  1.3000e-02,\n",
       "           -1.7700e-02, -2.4033e-04, -2.5482e-03, -8.2397e-03, -8.4229e-03,\n",
       "           -1.2634e-02,  3.0670e-03, -9.3384e-03, -1.6479e-02, -1.9531e-02,\n",
       "           -3.2715e-02, -4.5776e-03,  2.6855e-02, -3.4027e-03, -3.8452e-03,\n",
       "           -7.9346e-03, -4.8828e-03,  5.3101e-03,  2.7618e-03,  7.1716e-04,\n",
       "           -1.5991e-02, -1.2112e-04, -1.1902e-02,  5.8899e-03, -6.5613e-03,\n",
       "            9.8419e-04, -3.4523e-04,  8.6060e-03, -1.0315e-02,  4.6015e-05,\n",
       "            1.8555e-02,  5.6152e-03, -8.6670e-03, -1.2939e-02, -1.3611e-02,\n",
       "            9.7656e-03,  1.8799e-02,  7.1716e-03, -6.1340e-03,  7.2937e-03,\n",
       "            3.4523e-04, -1.3367e-02,  1.2451e-02,  1.5488e-03,  4.4861e-03,\n",
       "           -1.1108e-02]]], requires_grad=True),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook_data[i][forward_hook_module_names[0]][\"input\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77fda6d9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# plot_tensor_histogram(output_0 - input_1, is_show=True)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforward_hook_module_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m output_1_0 \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:260\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    261\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    262\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    263\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    264\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    265\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    266\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    267\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    268\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:166\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    164\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    167\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for i in range(1, 24):\n",
    "    # Skip the first inputs\n",
    "    input_0 = hook_data[i][forward_hook_module_names[0]][\"input\"][0][0]\n",
    "    output_0 = hook_data[i][forward_hook_module_names[0]][\"output\"][0][0]\n",
    "    input_1 = hook_data[i][forward_hook_module_names[1]][\"input\"][0][0]\n",
    "    output_1 = hook_data[i][forward_hook_module_names[0]][\"output\"][0][0]\n",
    "    # plot_tensor_histogram(output_0 - input_1, is_show=True)\n",
    "    module = eval(f\"model.{forward_hook_module_names[0]}\")\n",
    "    output_1_0 = module.forward(input_0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea6c11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 现在你可以正确调用decoder层\u001b[39;00m\n\u001b[0;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m handle\u001b[38;5;241m.\u001b[39mremove()  \u001b[38;5;66;03m# 记得移除hook\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:260\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    261\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    262\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    263\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    264\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    265\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    266\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    267\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    268\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\myopenai\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:166\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    164\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    167\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def register_decoder_hook(model, layer_idx):\n",
    "    hook_data = {\"input\": None, \"output\": None}\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        hook_data[\"input\"] = input\n",
    "        hook_data[\"output\"] = output\n",
    "    \n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook)\n",
    "    return hook_data, handle\n",
    "\n",
    "# 使用示例\n",
    "hook_data, handle = register_decoder_hook(model, 0)\n",
    "\n",
    "# 运行完整的前向传播\n",
    "input_ids = torch.tensor([[1, 2, 3]])  # 示例输入\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# 现在你可以获取完整的输入\n",
    "full_inputs = hook_data[\"input\"]\n",
    "hidden_states = full_inputs[0]\n",
    "attention_mask = full_inputs[1] if len(full_inputs) > 1 else None\n",
    "position_ids = full_inputs[2] if len(full_inputs) > 2 else None\n",
    "past_key_value = full_inputs[3] if len(full_inputs) > 3 else None\n",
    "position_embeddings = full_inputs[4] if len(full_inputs) > 4 else None\n",
    "\n",
    "# 现在你可以正确调用decoder层\n",
    "module = model.model.layers[0]\n",
    "output = module(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_value=past_key_value,\n",
    "    position_embeddings=position_embeddings\n",
    ")\n",
    "\n",
    "handle.remove()  # 记得移除hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2925bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 896])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_inputs[0].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myopenai_jupyter",
   "language": "python",
   "name": "myopenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
